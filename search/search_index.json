{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Llama4j","text":"<p>Llama4j is a collection of Java components designed to enable local LLM inference on the JVM. It provides all the essential building blocks - from tokenization to tensor operations - required to implement an efficient LLM inference engine in Java without external runtime dependencies.</p>"},{"location":"#overview","title":"Overview","text":"<p>Llama4j implements essential components for local LLM operations in pure Java, focusing on:</p> <ul> <li>Inference Engine: Core building blocks required to implement model inference</li> <li>Tokenization: Native tokenization utilities for processing text input</li> <li>GGUF File Format: API for reading and manipulating .GGUF files</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Pure Java Implementation: All components implemented in Java without external dependencies</li> <li>Local Execution: Run models directly on your hardware, on the JVM</li> <li>Low-Level Control: Direct access to model operations and memory management</li> <li>GGUF Format Support: Read and process GGUF files for model loading</li> <li>Custom Tokenization: Java implementation of commonly used tokenization algorithms</li> <li>Memory Efficient: Fine-grained control over model loading and memory usage</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<p>Llama4j is designed for:</p> <ul> <li>Building custom local LLM inference engines on the JVM</li> <li>Implementing specialized inference pipelines</li> <li>Scenarios requiring deep integration with Java systems</li> <li>Applications needing precise control over model operations</li> <li>Projects requiring offline LLM capabilities</li> </ul> <p>Llama4j provides low-level building blocks for LLM operations. It's designed for developers who need to implement their own inference engine or require precise control over model operations. If you're looking to quickly integrate LLMs into your Java application without dealing with low-level details,  consider using LangChain4j instead. LangChain4j provides a high-level API for working with various LLM providers and building AI-powered applications. Llama4j is better suited to implement a local inference backend for LangChain4j.</p>"},{"location":"gguf/","title":"GGUF API","text":"<p>GGUF is a binary format used by llama.cpp for storing models and their associated metadata and weights,  designed for efficient memory mapping of model weights, making it particularly suitable for large language models and other AI applications.</p> <p>See GGUF specification.</p> <p>The GGUF API provides methods to read and write GGUF files, access and manage metadata, handle tensor information, and support various data types and array structures.</p> <p>While the API manages the file structure and metadata, it does not directly write tensor data to the file. Users are responsible for writing tensor data at the correct file offsets specified by the API, ensuring proper data alignment and following the tensor specifications (type, dimensions, and layout) defined in the metadata.</p> <p>This design choice separates the concerns of file format management from the actual tensor data handling, allowing users to implement custom tensor writing strategies and optimize for their specific use cases. The API provides the necessary offset information and tensor specifications, but the actual tensor data writing must be handled by the user's implementation. For example, after creating a GGUF file with metadata and tensor definitions, users must:</p> <ul> <li>Get tensor offsets from the API via <code>GGUF#tensorDataOffset()</code> and <code>TensorInfo#offset()</code></li> <li>Seek to the correct file position</li> <li>Write tensor data in the correct format</li> <li>Ensure proper memory alignment according to <code>GGUF#getAlignment()</code></li> </ul>"},{"location":"gguf/#get-started","title":"Get Started","text":"<p>To use the GGUF library in your project, add the following dependency using your preferred build tool. The library requires Java 11 or higher and has no external dependencies.</p> MavenGradleSBTMill pom.xml<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.llama4j&lt;/groupId&gt;\n    &lt;artifactId&gt;gguf&lt;/artifactId&gt;\n    &lt;version&gt;0.1.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> build.gradle<pre><code>implementation 'com.llama4j:gguf:0.1.1'\n</code></pre> build.sbt<pre><code>libraryDependencies += \"com.llama4j\" % \"gguf\" % \"0.1.1\"\n</code></pre> build.sc<pre><code>ivy\"com.llama4j::gguf:0.1.1\"\n</code></pre>"},{"location":"gguf/#reading-gguf-files","title":"Reading GGUF files","text":"<pre><code>import com.llama4j.gguf.GGUF;\n\nPath modelPath = Path.of(\"/path/to/Llama-3.2-1B-Instruct-Q8_0.gguf\");\nGGUF gguf = GGUF.read(modelPath);\n\nSystem.out.println(gguf.getMetadataKeys());\n</code></pre> <p>Note</p> <p>When reading GGUF files, the order of metadata keys and tensors within the file is preserved.</p>"},{"location":"gguf/#basic-information","title":"Basic Information","text":"<pre><code>// Get GGUF format version\nint version = gguf.getVersion();\n\n// Get alignment value (default or specified)\nint alignment = gguf.getAlignment();\n\n// Get tensor data offset\nlong tensorDataOffset = gguf.getTensorDataOffset();\n</code></pre>"},{"location":"gguf/#accessing-metadata","title":"Accessing Metadata","text":"<pre><code>// Get all metadata keys\nSet&lt;String&gt; keys = gguf.getMetadataKeys();\n\n// Check if key exists\nboolean hasKey = gguf.containsKey(\"key\");\n\n// Get metadata type\nMetadataValueType type = gguf.getType(\"key\");\n\n// Get component type for arrays\nMetadataValueType arrayType = gguf.getComponentType(\"key\");\n\n// Get value with type casting\nint intValue = gguf.getValue(int.class, \"numberKey\");\nString text = gguf.getValue(String.class, \"textKey\");\nfloat[] floats = gguf.getValue(float[].class, \"floatArrayKey\");\n\n// Generic access if type is unknown\nObject value = gguf.getValue(Object.class, \"key\");\n\n// Get value with default fallback\nint theValue = gguf.getValueOrDefault(int.class, \"key\", 0);\n\n// String array\nString[] strings = gguf.getValue(String[].class, \"stringArray\");\n\n// Numeric arrays\nint[] integers = gguf.getValue(int[].class, \"intArray\");\nfloat[] floats = gguf.getValue(float[].class, \"floatArray\");\ndouble[] doubles = gguf.getValue(double[].class, \"doubleArray\");\n\n// Boolean array\nboolean[] bools = gguf.getValue(boolean[].class, \"boolArray\");\n</code></pre>"},{"location":"gguf/#accessing-tensors","title":"Accessing Tensors","text":"<pre><code>// Get all tensor information, tensor order is preserved\nCollection&lt;TensorInfo&gt; tensors = gguf.getTensors();\n\n// Get specific tensor info\nTensorInfo tensor = gguf.getTensor(\"token_embeddings\");\n\n// Check tensor existence\nboolean hasTensor = gguf.containsTensor(\"token_embeddings\");\n</code></pre>"},{"location":"gguf/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always close channels after reading/writing: <pre><code>try (var channel = Files.newByteChannel(path, StandardOpenOption.READ)) {\n    GGUF gguf = GGUF.read(channel);\n    // Process GGUF data\n}\n</code></pre></p> </li> <li> <p>Check for key/tensor existence before accessing values: <pre><code>if (gguf.containsKey(\"key\")) {\n    int value = gguf.getValue(int.class, \"key\");\n}\n// Or use default values\nfloat ropeTetha = gguf.getValueOrDefault(float.class, \"key\", 1e-5f);\n</code></pre></p> </li> </ol>"},{"location":"gguf/#reading-from-urls","title":"Reading from URLs","text":"<p>No need to download large GGUF files or using a browser to peek at the GGUF metadata - GGUF metadata can be easily read from various sources, including URLs.</p> <pre><code>static GGUF readFromHuggingFace(String user, String repo, String file) throws IOException {\n    URL url = new URL(\"https://hf.co/%s/%s/resolve/main/%s\".formatted(user, repo, file));\n    try (var ch = Channels.newChannel(\n            new BufferedInputStream(url.openConnection().getInputStream()))) {\n        return GGUF.read(ch);\n    }\n}\n\n// Example usage\nString user = \"mukel\";\nString repo = \"Llama-3.2-1B-Instruct-GGUF\";\nString file = \"Llama-3.2-1B-Instruct-Q8_0.gguf\";\nGGUF gguf = readFromHuggingFace(user, repo, file);\n\nSystem.out.println(gguf.getMetadataKeys());\n</code></pre>"},{"location":"gguf/#writing-gguf-files","title":"Writing GGUF files","text":"<pre><code>// Write to file\nGGUF gguf = ...;\nGGUF.write(gguf, Path.of(\"model.gguf\"));\n</code></pre> <p>The Builder interface provides a fluent API for creating and modifying GGUF metadata.</p>"},{"location":"gguf/#creating-a-new-gguf-file","title":"Creating a New GGUF File","text":"<pre><code>// Create a new empty builder\nBuilder builder = Builder.newBuilder()\n    .setVersion(3)     // (optionsl) set GGUF version\n    .setAlignment(32); // (optional) set alignment (must be power of 2)\n\n// Add metadata\nbuilder\n    .putString(\"name\", \"my-model\")\n    .putInteger(\"num_layers\", 32)\n    .putFloat(\"learning_rate\", 0.001f)\n    .putArrayOfString(\"vocab\", new String[]{\"token1\", \"token2\"});\n\n// Add tensor information\nbuilder.putTensor(\n    TensorInfo.create(\n        \"weights\",    // tensor name\n        new long[]{1024, 1024}, // shape\n        GGMLType.F32, // GGML type\n        offset        // tensor offset w.r.t. to gguf.tensorDataOffset()\n));\n\n// Build the GGUF instance\nGGUF gguf = builder.build();\n\n// Write to file\nGGUF.write(gguf, Path.of(\"model.gguf\"));\n</code></pre>"},{"location":"gguf/#modifying-existing-gguf-files","title":"Modifying Existing GGUF Files","text":"<pre><code>// Create builder from existing GGUF\nGGUF existing = GGUF.read(Path.of(\"model.gguf\"));\nBuilder builder = Builder.newBuilder(existing);\n\n// Modify metadata\nbuilder\n    .putString(\"description\", \"Updated model\")\n    .removeKey(\"old_key\")\n    .putInteger(\"num_layers\", 64);\n\n// Modify tensors\nbuilder\n    .removeTensor(\"old_tensor\")\n    .putTensor(TensorInfo.create(\"new_tensor\", ...));\n\n// Build and save\nGGUF updated = builder.build();\nGGUF.write(updated, Path.of(\"updated-model.gguf\"));\n</code></pre>"},{"location":"gguf/#re-compute-tensors-offsets","title":"Re-compute tensors offsets","text":"<p>By default, the tensor offsets are re-computed on <code>.build()</code>.</p> <pre><code>// Tensors are packed in the same order, respecting the alignment\nGGUF gguf1 = builder.build(); // recomputeTensorOffsets = true\n\n// Leave tensors as-is, do not re-compute tensors offsets\nGGUF gguf2 = builder.build(false); // recomputeTensorOffsets = false\n</code></pre> <p>Warning</p> <p>When <code>recomputeTensorOffsets</code> is false, you must ensure tensor offsets are correctly set manually.</p>"},{"location":"gguf/#metadata-types","title":"Metadata Types","text":"<p>The API supports various data types for metadata values, unsigned types are always stored using Java signed types.</p> GGUF Type Java Type Description INT8 byte Signed 8-bit integer UINT8 byte Unsigned 8-bit integer (requires manual conversion) INT16 short Signed 16-bit integer UINT16 short Unsigned 16-bit integer (requires manual conversion) INT32 int Signed 32-bit integer UINT32 int Unsigned 32-bit integer (requires manual conversion) INT64 long Signed 64-bit integer UINT64 long Unsigned 64-bit integer (requires manual conversion) FLOAT32 float 32-bit floating point FLOAT64 double 64-bit floating point BOOL boolean Boolean value STRING String Text string ARRAY String[] or primitive arrays Array of supported types <p>Warning</p> <p>Reading/writing arrays of arrays is currently NOT supported.</p> <pre><code>Builder.newBuilder()\n    .putBoolean(\"flag\", true)\n    .putByte(\"byte_val\", (byte) 1)\n    .putShort(\"short_val\", (short) 100)\n    .putInteger(\"int_val\", 1000)\n    .putLong(\"long_val\", 10000L)\n    .putFloat(\"float_val\", 0.5f)\n    .putDouble(\"double_val\", 0.75)\n    .putBoolean(\"string_val\", \"hello\")\n    // Unsigned types\n    .putUnsignedByte(\"ubyte\", (byte) 255)\n    .putUnsignedShort(\"ushort\", (short) 65535)\n    .putUnsignedInteger(\"uint\", 0xFFFFFFFF)\n    .putUnsignedLong(\"ulong\", -1L)  // Represents max unsigned long\n    // Arrays\n    .putArrayOfString(\"strings\", new String[]{\"a\", \"b\", \"c\"})\n    .putArrayOfInteger(\"ints\", new int[]{1, 2, 3})\n    .putArrayOfFloat(\"floats\", new float[]{0.1f, 0.2f, 0.3f})    \n    .putArrayOfBoolean(\"flags\", new boolean[]{true, false, true});\n</code></pre>"}]}